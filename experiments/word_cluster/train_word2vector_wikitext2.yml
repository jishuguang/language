save:
  dir: D:\data\train\wikitext2

model:
  name: skip_gram
  embedding_dim: 128

pretrain:
#  load:
#  resume:

data:
  type: word2vector
  name: wikitext2
  data_path: D:\data\origin
  vocab_path: D:\data\vocab\wikitext2_vocab.pt
  min_freq: 10
  specials: ['<unk>']
  ignore_case: True
  window: 5
  negative_amount: 5

trainer:
  device:
    name: 'cuda:0'
    batch_size: 512
    num_worker: 0
  learn:
    method: Adam
    lr: 0.01
    epochs: 10
    milestones: [4,8]
